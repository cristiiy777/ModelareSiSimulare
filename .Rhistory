#or the length of replicate
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (a0 + (a1 * x[i-1])) %% m
}
return(x)
}
lcg_1 = lcg(a0 = 1467945236, a1 = 2698231, m = 2^32 , x0 = 2768923, n = 1000)
lcg_1
lcg_2 = lcg(a0 = 2467891211, a1 = 3579842, m = 2^29 , x0 = 4567742, n = 1500)
lcg_2
#parameters for MCG
a1 = 27
m = 256
x0 = 201
n = 71
mcg = function(a1, m, x0, n)
{
x = rep(0,n+1)
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (a1 * x[i-1]) %% m
}
return(x)
}
r = mcg(a1, m, x0, n)
r
mcg_1 = mcg(a1 = 176, m = 255, x0 = 301, n = 300)
mcg_1
mcg_2 = mcg(a1 = 27, m = 256, x0 = 201, n = 300)
mcg_2
mcg_2 = mcg(a1 = 27, m = 256, x0 = 201, n = 300)
mcg_2
mcg = function(a1, m, x0, n)
{
x = rep(0,n+1)
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (a1 * x[i-1]) %% m
}
return(x)
}
mcg_1 = mcg(a1 = 261, m = 155, x0 = 91, n = 500)
mcg_1
mcg_2 = mcg(a1 = 27, m = 256, x0 = 201, n = 300)
b = rep(0, n+1)
mcg = function(a1, m, x0, n)
{
x = rep(0,n+1)
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (a1 * x[i-1]) %% m
}
return(x)
}
mcg_1 = mcg(a1 = 261, m = 155, x0 = 91, n = 500)
mcg_1
mcg_2 = mcg(a1 = 2, m = 31, x0 = 187, n = 300)
mcg_2
mcg_2 = mcg(a1 = 233, m = 31, x0 = 187, n = 300)
mcg_2
mcg_2 = mcg(a1 = 233, m = 121, x0 = 187, n = 300)
mcg_2
mcg_1 = mcg(a1 = 261, m = 155, x0 = 91, n = 700)
mcg_1
mcg_2 = mcg(a1 = 233, m = 121, x0 = 187, n = 500)
mcg_2
#RANDU (a special case of MCG)
randu = function(x0, n)
{
x = rep(0,n+1)
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (65539 * x[i-1]) %% (2^31)
}
return(x[-1])
}
randu(2,20)
randu_2 <- randu(2,20)
randu_2
randu = function(x0, n)
{
x = rep(0,n+1)
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (65539 * x[i-1]) %% (2^31)
}
return(x[-1])
}
randu_1 <- randu(2,700)
randu_1
randu_1 <- randu(233,700)
randu_1
randu = function(x0, n)
{
x = rep(0,n+1)
x[1] = x0
for(i in 1:n+1)
{
if(i == 1)
x[i] = x0
else
x[i] = (567313 * x[i-1]) %% (2^37)
}
return(x[-1])
}
randu_2 <- randu(16,800)
randu_2
barplot(lcg_1)
barplot(lcg_2)
mcg_1
barplot(mcg_1)
barplot(mcg_2)
barplot(randu_1)
barplot(randu_2)
hist(lcg_1)
hist(lcg_2)
hist(mcg_1)
hist(mcg_2)
hist(randu_1)
hist(randu_2)
lcg_2 <- cg(a0 =  2467891211, a1 = 3579842, m = 2^31 , x0 = 4567742, n = 800)
lcg_2
barplot(lcg_2)
hist(lcg_2)
lcg_2 <- cg(a0 =  2467891211, a1 = 3579842, m = 2^32 , x0 = 4567742, n = 800)
lcg_2
lcg_2 <- cg(a0 =  1972891241, a1 = 2579842, m = 2^32 , x0 = 3567742, n = 800)
lcg_2
lcg_2 <- cg(a0 =  1972891241, a1 = 2579842, m = 2^32 , x0 = 3567742, n = 800)
lcg_2
hist(lcg_2)
barplot(lcg_2)
barplot(lcg_1)
hist(randu_1)
hist(randu_2)
lcg_2 <- lcg(a0 =  1972891241, a1 = 2579842, m = 2^32 , x0 = 3567742, n = 800)
lcg_2
barplot(lcg_1)
barplot(lcg_2)
hist(lcg_2)
lcg_2 <- lcg(a0 = 1972891241, a1 = 2579842, m = 2^32 , x0 = 3567742, n = 400)
lcg_2
lcg_1 <- lcg(a0 = 1467945236, a1 = 2698231, m = 2^32 , x0 = 2768923, n = 500)
lcg_1
lcg_2 <- lcg(a0 = 1972891241, a1 = 2579842, m = 2^32 , x0 = 3567742, n = 400)
lcg_2
barplot(lcg_1)
barplot(lcg_2)
hist(lcg_1)
hist(lcg_2)
hist(randu_1)
hist(randu_2)
ptestg(lcg_1, method = c("Fisher", "robust", "extended", "extendedRobust",
"FisherRSR"), multiple = FALSE)
periodicity(lcg_1, ...)
acf(randu_1)
ks.test(lcg_1,'punif')
chisq.unif.test(lcg_1)
acf(lcg_1)
acf(lcg_1)
acf(lcg_2)
acf(mcg_1)
acf(mcg_2)
acf(randu_1)
acf(randu_2)
acf(mcg_1)
chisq.unif.test(u)chisq.unif.test(u)
x <- rexp(n=100, rate=1)
x
x <- rexp(n=100, rate=0.5)
x
curve(x)
curve(rexp(n=100, rate=0.5))
curve(x, rexp(n=100, rate=0.5))
curve(rexp(x))
curve(pexp(x, 0.5), col = 'blue')
x <- rexp(n=100, rate=0.5)
curve(rexp(x))
x <- rexp(n=100, rate=1)
curve(rexp(x))
x <- rexp(n=100, rate=2)
curve(rexp(x))
x <- rexp(n=100, rate=10)
curve(rexp(x))
x <- rexp(n=100, rate=10)
curve(rexp(x))
curve(dgamma(x, shape = 1, scale = 2))
x <- rgamma(n=100, shape = 1, scale = 2)
curve(rgamma(x))
x <- rgamma(n=100, scale = 2, shape = 1)
curve(rgamma(x))
x <- rgamma(n=10, scale=1, shape=0.4)
curve(rgamma(x))
x <- rgamma(n=10, scale=1, shape=0.4)
curve(rgamma(x, scale=1, shape=0.4))
x <- rgamma(n=10, shape= 1, scale = 2)
curve(rgamma(x, shape = 1, scale = 2))
curve(dt(x, 10), lwd=3, col= "blue")
curve(dt(x=10, 10), lwd=3, col= "blue")
x <- dt(100, 10)
x
x <- dt(100, 10)
curve(dt(x, 10), lwd=3, col= "blue")
curve(qt(.975,x), from = 2 , to = 100, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 10, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 5, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 5, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 2, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 1, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(dt(x, 2), lwd=3, col= "blue")
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
norm
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
norm       <- read.csv(file = fisierDeer, header = TRUE)
norm
norm
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
normT       <- read.csv(file = fisierDeer, header = TRUE)
normT
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
norm       <- read.csv(file = fisierNorm, header = TRUE)
norm
hist(norm$Process1)
hist(norm$Process2)
hist(norm$Process1)
ecdf(norm)
ecdf(norm$Process1)
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process2))
plot(ecdf(norm$Process2))
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process2))
ks.test(norm$Process1, norm$Process2)
plot(ks.test(norm$Process1, norm$Process2))
plot(norm$Process1, norm$Process2, type="l",col="red")
plot(norm$Process1, , type="l",col="red")
plot(norm$Process1, type="l",col="red")
plot(norm$Process1, type="l",col="red")
ks.test(norm$Process1, norm$Process2)
plot(ecdf(norm$Process1), ecdf(norm$Process2) type="l",col="red")
plot(ecdf(norm$Process1), ecdf(norm$Process2), type="l",col="red")
plot(ecdf(norm$Process1), type="l",col="red")
plot(ecdf(norm$Process1), ,type="l",col="red")
plot(ecdf(norm$Process1),type="l",col="red")
plot(norm$Process1,type="l",col="red")
plot(norm$Process1, norm$Process2 ,type="l",col="red")
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process2))
ks.test(norm$Process1, norm$Process2)
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.lm
coeffs = coefficients(eruption.lm); coeffs
coeffs = coefficients(eruption.lm)
coeffs
waiting = 80           # the waiting time
duration = coeffs[1] + coeffs[2]*waiting
duration
newdata = data.frame(waiting=80) # wrap the parameter
#Then we apply the predict function to eruption.lm along with newdata.
predict(eruption.lm, newdata)    # apply predict
eruption.lm = lm(eruptions ~ waiting, data=faithful)
#Then we extract the coefficient of determination from the r.squared attribute of its summary.
summary(eruption.lm)$r.squared
eruption.lm = lm(eruptions ~ waiting, data=faithful)
#Then we print out the F-statistics of the significance test with the summary function.
summary(eruption.lm)
#We apply the lm function to a formula that describes the variable eruptions
#by the variable waiting, and save the linear regression model in a
#new variable eruption.lm. Then we compute the residual with the resid function.
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.res = resid(eruption.lm)
#We now plot the residual against the observed values of the variable waiting.
plot(faithful$waiting, eruption.res,
+     ylab="Residuals", xlab="Waiting Time",
+     main="Old Faithful Eruptions")
abline(0, 0)                  # the horizon
#Assume that the error term Ïµ in the simple linear regression
#model is independent of x, and is normally distributed, with
#zero mean and constant variance. For a given value of x,
#the interval estimate of the dependent variable y is called the
#prediction interval.
#In the data set faithful, develop a 95% prediction interval of
#the eruption duration for the waiting time of 80 minutes.
#We apply the lm function to a formula that describes the
#variable eruptions by the variable waiting, and save the
#liniar regression model in a new variable eruption.lm.
attach(faithful)     # attach the data frame
eruption.lm = lm(eruptions ~ waiting)
#Then we create a new data frame that set the waiting time value.
newdata = data.frame(waiting=80)
#We now apply the predict function and set the predictor variable
#in the newdata argument. We also set the interval type as "predict", and use the default 0.95 confidence level.
predict(eruption.lm, newdata, interval="predict")
plot(faithful$waiting, eruption.res, ylab="Residuals", xlab="Waiting Time", main="Old Faithful Eruptions")
abline(0, 0)                  # the horizon
qqnorm(eruption.stdres, ylab="Standardized Residuals", xlab="Normal Scores", main="Old Faithful Eruptions")
qqline(eruption.stdres)
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.stdres = rstandard(eruption.lm)
#We now create the normal probability plot with the qqnorm function,
#and add the qqline for further comparison.
qqnorm(eruption.stdres, ylab="Standardized Residuals", xlab="Normal Scores", main="Old Faithful Eruptions")
qqline(eruption.stdres)
cars
plot(cars$speed)
plot(cars$dist)
boxplot(cars$speed)
boxplot(cars$dist)
density(cars$speed)
density(cars$dist)
density(cars$speed)
viteza   <- cars$speed
distanta <- cars$dist
cor(distanta, viteza)
plot(cars$speed)
scatter.smooth(cars$speed)
scatter.smooth(cars$dist)
linearMod <- lm(dist ~ speed, data=cars)  # build linear regression model on full data
linearMod
boxplot(cars$speed)
summary(modelLiniar)
modelLiniar <- lm(dist ~ speed, data=cars) #model de regresie liniara
summary(modelLiniar)
summary(modelLiniar)
AIC(modelLiniar)  # AIC => 419.1569
BIC(modelLiniar)  # BIC => 424.8929
scatter.smooth(cars$speed)
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data
testData
lmMod <- lm(dist ~ speed, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance
distPred
summary (lmMod)  # model summary
actuals_preds <- data.frame(cbind(actuals=testData$dist, predicteds=distPred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)  # 82.7%
head(actuals_preds)
scatter.smooth(actuals_preds)
actuals_preds
#A multiple linear regression (MLR) model that describes a dependent variable y by independent
#variables x1, x2, ..., xp (p > 1) is expressed by the equation as follows, where the numbers
#Î± and Î²k (k = 1, 2, ..., p) are the parameters, and Ïµ is the error term.
#For example, in the built-in data set stackloss from observations of a chemical plant operation
#, if we assign stackloss as the dependent variable, and assign Air.Flow (cooling air flow),
#Water.Temp (inlet water temperature) and Acid.Conc. (acid concentration) as independent variables
#__Estimated Multiple Regression Equation
#If we choose the parameters Î± and Î²k (k = 1, 2, ..., p) in the multiple linear regression model
#so as to minimize the sum of squares of the error term Ïµ, we will have the so called estimated
#multiple regression equation. It allows us to compute fitted values of y based
#on a set of values of xk (k = 1, 2, ..., p) .
#Apply the multiple linear regression model for the data set stackloss, and predict the stack
#loss if the air flow is 72, water temperature is 20 and acid concentration is 85.
#We apply the lm function to a formula that describes the variable stack.loss by the variables
#Air.Flow, Water.Temp and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#We also wrap the parameters inside a new data frame named newdata.
newdata = data.frame(Air.Flow=72, Water.Temp=20, Acid.Conc.=85) # wrap the parameters
#Lastly, we apply the predict function to stackloss.lm and newdata.
predict(stackloss.lm, newdata)
#__Multiple Coefficient of Determination
#The coefficient of determination of a multiple linear regression model is the quotient of the
#variances of the fitted values and observed values of the dependent variable. If we denote yi
#as the observed values of the dependent variable, Â¯y as its mean, and yËi as the fitted value,
#then the coefficient of determination is:
#Find the coefficient of determination for the multiple linear regression model of the data set stackloss.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow,
#Water.Temp and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#Then we extract the coefficient of determination from the r.squared attribute of its summary.
summary(stackloss.lm)$r.squared
#The coefficient of determination of the multiple linear regression model for the data set stackloss is 0.91358.
#__Adjusted Coefficient of Determination
#The adjusted coefficient of determination of a multiple linear regression model is defined in terms of
#the coefficient of determination as follows, where n is the number of observations in the data set, and
#p is the number of independent variables.
#Find the adjusted coefficient of determination for the multiple linear regression model of the data set
#stackloss.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow,
#Water.Temp and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#Then we extract the coefficient of determination from the adj.r.squared attribute of its summary.
summary(stackloss.lm)$adj.r.squared
#The adjusted coefficient of determination of the multiple linear regression model for the data set stackloss is 0.89833.
#__Significance Test for MLR
#Assume that the error term Ïµ in the multiple linear regression (MLR) model is independent of xk (k = 1, 2, ..., p),
#and is normally distributed, with zero mean and constant variance. We can decide whether there is any significant
#relationship between the dependent variable y and any of the independent variables xk (k = 1, 2, ..., p).
#Decide which of the independent variables in the multiple linear regression model of the data set stackloss are
#statistically significant at .05 significance level.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow, Water.Temp
#and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#The t values of the independent variables can be found with the summary function.
summary(stackloss.lm)
#As the p-values of Air.Flow and Water.Temp are less than 0.05, they are both statistically significant
#in the multiple linear regression model of stackloss.
#__Confidence Interval for MLR
#Assume that the error term Ïµ in the multiple linear regression (MLR) model is independent of xk (k = 1, 2, ..., p),
#and is normally distributed, with zero mean and constant variance. For a given set of values of xk (k = 1, 2, ..., p), the interval estimate for the mean of the dependent variable, Â¯y , is called the confidence interval.
#In data set stackloss, develop a 95% confidence interval of the stack loss if the air flow is 72, water temperature
#is 20 and acid concentration is 85.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow, Water.Temp
#and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
attach(stackloss)    # attach the data frame
stackloss.lm = lm(stack.loss ~
+     Air.Flow + Water.Temp + Acid.Conc.)
#Then we wrap the parameters inside a new data frame variable newdata.
newdata = data.frame(Air.Flow=72,
+     Water.Temp=20,
+     Acid.Conc.=85)
#We now apply the predict function and set the predictor variable in the newdata argument. We also set the interval
#type as "confidence", and use the default 0.95 confidence level.
predict(stackloss.lm, newdata, interval="confidence")
#The 95% confidence interval of the stack loss with the given parameters is between 20.218 and 28.945.
#__Prediction Interval for MLR
#Assume that the error term Ïµ in the multiple linear regression (MLR) model is independent of xk (k = 1, 2, ..., p),
#and is normally distributed, with zero mean and constant variance. For a given set of values of xk (k = 1, 2, ..., p),
#the interval estimate of the dependent variable y is called the prediction interval.
#In data set stackloss, develop a 95% prediction interval of the stack loss if the air flow is 72, water temperature
#is 20 and acid concentration is 85.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow, Water.Temp
#and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
attach(stackloss)    # attach the data frame
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc.)
newdata = data.frame(Air.Flow=72, Water.Temp=20, Acid.Conc.=85)
#We now apply the predict function and set the predictor variable in the newdata argument. We also set the interval
#type as "predict", and use the default 0.95 confidence level.
predict(stackloss.lm, newdata, interval="predict")
detach(stackloss)    # clean up
#The 95% confidence interval of the stack loss with the given parameters is between 16.466 and 32.697.
chisq.test(lcg_1)
chisq.test(lcg_1)
chisq.test(lcg_2)
chisq.test(mcg_1)
chisq.test(mcg_2)
chisq.test(randu_1)
chisq.test(randu_2)
n0 = rnorm(100,0.1)
n1 = rnorm(500,0.1)
n2 = rnorm(1000,0.1)
n3 = rnorm(5000,0.1)
n4 = rnorm(10000,0.1)
n5 = rnorm(100000,0.1)
n6 = rnorm(1000000,0.1)
hist(n0)
hist(n1)
hist(n2)
hist(n3)
hist(n4)
hist(n5)
hist(n6)
hist(dnorm(n2,0,1))
hist(dnorm(n0+n1+n2+n3+n4+n5+n6,0,1))
pnorm(27.4, mean=50, sd=20)
hist(rnorm(n = 100, mean = 0, sd = 1))
hist(rnorm(n = 500, mean = 0, sd = 1))
hist(rnorm(n = 1000, mean = 0, sd = 1))
hist(rnorm(n = 5000, mean = 0, sd = 1))
hist(rnorm(n = 10000, mean = 0, sd = 1))
hist(rnorm(n = 100000, mean = 0, sd = 1))
hist(rnorm(n = 1000000, mean = 0, sd = 1))
#Folosind functia dnorm(), realizati graficul functiei de densitate de probabilitate a v.a. N(0,1).
curve(dnorm(x),
xlim = c(-4, 4),
ylab = "Density",
main = "Standard Normal Density Function")
#Trasati acest grafic peste histogramele obtinute la pasul anterior.
h <- hist(rnorm(n = 100, mean = 0, sd = 1))
x <- rnorm(n = 100, mean = 0, sd = 1)
xfit<-seq(min(x),max(x),length=4)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
