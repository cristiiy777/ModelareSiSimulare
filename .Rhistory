x <- rexp(n=100, rate=10)
curve(rexp(x))
x <- rexp(n=100, rate=10)
curve(rexp(x))
curve(dgamma(x, shape = 1, scale = 2))
x <- rgamma(n=100, shape = 1, scale = 2)
curve(rgamma(x))
x <- rgamma(n=100, scale = 2, shape = 1)
curve(rgamma(x))
x <- rgamma(n=10, scale=1, shape=0.4)
curve(rgamma(x))
x <- rgamma(n=10, scale=1, shape=0.4)
curve(rgamma(x, scale=1, shape=0.4))
x <- rgamma(n=10, shape= 1, scale = 2)
curve(rgamma(x, shape = 1, scale = 2))
curve(dt(x, 10), lwd=3, col= "blue")
curve(dt(x=10, 10), lwd=3, col= "blue")
x <- dt(100, 10)
x
x <- dt(100, 10)
curve(dt(x, 10), lwd=3, col= "blue")
curve(qt(.975,x), from = 2 , to = 100, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 10, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 5, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 5, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 2, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(qt(.975,x), from = 2 , to = 1, ylab = "Quantile 0.975 ", xlab = "Degrees of freedom", main = "Student t distribution")
abline(h=qnorm(.975), col = 2)
curve(dt(x, 2), lwd=3, col= "blue")
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
norm
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
norm       <- read.csv(file = fisierDeer, header = TRUE)
norm
norm
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
normT       <- read.csv(file = fisierDeer, header = TRUE)
normT
directory  <- "C:/Users/Lenovo/Desktop/ModelareSiSimulare"
fisierNorm <- file.path(directory, "Norm.t.data_.csv")
norm       <- read.csv(file = fisierNorm, header = TRUE)
norm
hist(norm$Process1)
hist(norm$Process2)
hist(norm$Process1)
ecdf(norm)
ecdf(norm$Process1)
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process2))
plot(ecdf(norm$Process2))
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process2))
ks.test(norm$Process1, norm$Process2)
plot(ks.test(norm$Process1, norm$Process2))
plot(norm$Process1, norm$Process2, type="l",col="red")
plot(norm$Process1, , type="l",col="red")
plot(norm$Process1, type="l",col="red")
plot(norm$Process1, type="l",col="red")
ks.test(norm$Process1, norm$Process2)
plot(ecdf(norm$Process1), ecdf(norm$Process2) type="l",col="red")
plot(ecdf(norm$Process1), ecdf(norm$Process2), type="l",col="red")
plot(ecdf(norm$Process1), type="l",col="red")
plot(ecdf(norm$Process1), ,type="l",col="red")
plot(ecdf(norm$Process1),type="l",col="red")
plot(norm$Process1,type="l",col="red")
plot(norm$Process1, norm$Process2 ,type="l",col="red")
plot(ecdf(norm$Process1))
plot(ecdf(norm$Process2))
ks.test(norm$Process1, norm$Process2)
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.lm
coeffs = coefficients(eruption.lm); coeffs
coeffs = coefficients(eruption.lm)
coeffs
waiting = 80           # the waiting time
duration = coeffs[1] + coeffs[2]*waiting
duration
newdata = data.frame(waiting=80) # wrap the parameter
#Then we apply the predict function to eruption.lm along with newdata.
predict(eruption.lm, newdata)    # apply predict
eruption.lm = lm(eruptions ~ waiting, data=faithful)
#Then we extract the coefficient of determination from the r.squared attribute of its summary.
summary(eruption.lm)$r.squared
eruption.lm = lm(eruptions ~ waiting, data=faithful)
#Then we print out the F-statistics of the significance test with the summary function.
summary(eruption.lm)
#We apply the lm function to a formula that describes the variable eruptions
#by the variable waiting, and save the linear regression model in a
#new variable eruption.lm. Then we compute the residual with the resid function.
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.res = resid(eruption.lm)
#We now plot the residual against the observed values of the variable waiting.
plot(faithful$waiting, eruption.res,
+     ylab="Residuals", xlab="Waiting Time",
+     main="Old Faithful Eruptions")
abline(0, 0)                  # the horizon
#Assume that the error term ϵ in the simple linear regression
#model is independent of x, and is normally distributed, with
#zero mean and constant variance. For a given value of x,
#the interval estimate of the dependent variable y is called the
#prediction interval.
#In the data set faithful, develop a 95% prediction interval of
#the eruption duration for the waiting time of 80 minutes.
#We apply the lm function to a formula that describes the
#variable eruptions by the variable waiting, and save the
#liniar regression model in a new variable eruption.lm.
attach(faithful)     # attach the data frame
eruption.lm = lm(eruptions ~ waiting)
#Then we create a new data frame that set the waiting time value.
newdata = data.frame(waiting=80)
#We now apply the predict function and set the predictor variable
#in the newdata argument. We also set the interval type as "predict", and use the default 0.95 confidence level.
predict(eruption.lm, newdata, interval="predict")
plot(faithful$waiting, eruption.res, ylab="Residuals", xlab="Waiting Time", main="Old Faithful Eruptions")
abline(0, 0)                  # the horizon
qqnorm(eruption.stdres, ylab="Standardized Residuals", xlab="Normal Scores", main="Old Faithful Eruptions")
qqline(eruption.stdres)
eruption.lm = lm(eruptions ~ waiting, data=faithful)
eruption.stdres = rstandard(eruption.lm)
#We now create the normal probability plot with the qqnorm function,
#and add the qqline for further comparison.
qqnorm(eruption.stdres, ylab="Standardized Residuals", xlab="Normal Scores", main="Old Faithful Eruptions")
qqline(eruption.stdres)
cars
plot(cars$speed)
plot(cars$dist)
boxplot(cars$speed)
boxplot(cars$dist)
density(cars$speed)
density(cars$dist)
density(cars$speed)
viteza   <- cars$speed
distanta <- cars$dist
cor(distanta, viteza)
plot(cars$speed)
scatter.smooth(cars$speed)
scatter.smooth(cars$dist)
linearMod <- lm(dist ~ speed, data=cars)  # build linear regression model on full data
linearMod
boxplot(cars$speed)
summary(modelLiniar)
modelLiniar <- lm(dist ~ speed, data=cars) #model de regresie liniara
summary(modelLiniar)
summary(modelLiniar)
AIC(modelLiniar)  # AIC => 419.1569
BIC(modelLiniar)  # BIC => 424.8929
scatter.smooth(cars$speed)
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data
testData
lmMod <- lm(dist ~ speed, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance
distPred
summary (lmMod)  # model summary
actuals_preds <- data.frame(cbind(actuals=testData$dist, predicteds=distPred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)  # 82.7%
head(actuals_preds)
scatter.smooth(actuals_preds)
actuals_preds
#A multiple linear regression (MLR) model that describes a dependent variable y by independent
#variables x1, x2, ..., xp (p > 1) is expressed by the equation as follows, where the numbers
#α and βk (k = 1, 2, ..., p) are the parameters, and ϵ is the error term.
#For example, in the built-in data set stackloss from observations of a chemical plant operation
#, if we assign stackloss as the dependent variable, and assign Air.Flow (cooling air flow),
#Water.Temp (inlet water temperature) and Acid.Conc. (acid concentration) as independent variables
#__Estimated Multiple Regression Equation
#If we choose the parameters α and βk (k = 1, 2, ..., p) in the multiple linear regression model
#so as to minimize the sum of squares of the error term ϵ, we will have the so called estimated
#multiple regression equation. It allows us to compute fitted values of y based
#on a set of values of xk (k = 1, 2, ..., p) .
#Apply the multiple linear regression model for the data set stackloss, and predict the stack
#loss if the air flow is 72, water temperature is 20 and acid concentration is 85.
#We apply the lm function to a formula that describes the variable stack.loss by the variables
#Air.Flow, Water.Temp and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#We also wrap the parameters inside a new data frame named newdata.
newdata = data.frame(Air.Flow=72, Water.Temp=20, Acid.Conc.=85) # wrap the parameters
#Lastly, we apply the predict function to stackloss.lm and newdata.
predict(stackloss.lm, newdata)
#__Multiple Coefficient of Determination
#The coefficient of determination of a multiple linear regression model is the quotient of the
#variances of the fitted values and observed values of the dependent variable. If we denote yi
#as the observed values of the dependent variable, ¯y as its mean, and yˆi as the fitted value,
#then the coefficient of determination is:
#Find the coefficient of determination for the multiple linear regression model of the data set stackloss.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow,
#Water.Temp and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#Then we extract the coefficient of determination from the r.squared attribute of its summary.
summary(stackloss.lm)$r.squared
#The coefficient of determination of the multiple linear regression model for the data set stackloss is 0.91358.
#__Adjusted Coefficient of Determination
#The adjusted coefficient of determination of a multiple linear regression model is defined in terms of
#the coefficient of determination as follows, where n is the number of observations in the data set, and
#p is the number of independent variables.
#Find the adjusted coefficient of determination for the multiple linear regression model of the data set
#stackloss.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow,
#Water.Temp and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#Then we extract the coefficient of determination from the adj.r.squared attribute of its summary.
summary(stackloss.lm)$adj.r.squared
#The adjusted coefficient of determination of the multiple linear regression model for the data set stackloss is 0.89833.
#__Significance Test for MLR
#Assume that the error term ϵ in the multiple linear regression (MLR) model is independent of xk (k = 1, 2, ..., p),
#and is normally distributed, with zero mean and constant variance. We can decide whether there is any significant
#relationship between the dependent variable y and any of the independent variables xk (k = 1, 2, ..., p).
#Decide which of the independent variables in the multiple linear regression model of the data set stackloss are
#statistically significant at .05 significance level.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow, Water.Temp
#and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data=stackloss)
#The t values of the independent variables can be found with the summary function.
summary(stackloss.lm)
#As the p-values of Air.Flow and Water.Temp are less than 0.05, they are both statistically significant
#in the multiple linear regression model of stackloss.
#__Confidence Interval for MLR
#Assume that the error term ϵ in the multiple linear regression (MLR) model is independent of xk (k = 1, 2, ..., p),
#and is normally distributed, with zero mean and constant variance. For a given set of values of xk (k = 1, 2, ..., p), the interval estimate for the mean of the dependent variable, ¯y , is called the confidence interval.
#In data set stackloss, develop a 95% confidence interval of the stack loss if the air flow is 72, water temperature
#is 20 and acid concentration is 85.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow, Water.Temp
#and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
attach(stackloss)    # attach the data frame
stackloss.lm = lm(stack.loss ~
+     Air.Flow + Water.Temp + Acid.Conc.)
#Then we wrap the parameters inside a new data frame variable newdata.
newdata = data.frame(Air.Flow=72,
+     Water.Temp=20,
+     Acid.Conc.=85)
#We now apply the predict function and set the predictor variable in the newdata argument. We also set the interval
#type as "confidence", and use the default 0.95 confidence level.
predict(stackloss.lm, newdata, interval="confidence")
#The 95% confidence interval of the stack loss with the given parameters is between 20.218 and 28.945.
#__Prediction Interval for MLR
#Assume that the error term ϵ in the multiple linear regression (MLR) model is independent of xk (k = 1, 2, ..., p),
#and is normally distributed, with zero mean and constant variance. For a given set of values of xk (k = 1, 2, ..., p),
#the interval estimate of the dependent variable y is called the prediction interval.
#In data set stackloss, develop a 95% prediction interval of the stack loss if the air flow is 72, water temperature
#is 20 and acid concentration is 85.
#We apply the lm function to a formula that describes the variable stack.loss by the variables Air.Flow, Water.Temp
#and Acid.Conc. And we save the linear regression model in a new variable stackloss.lm.
attach(stackloss)    # attach the data frame
stackloss.lm = lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc.)
newdata = data.frame(Air.Flow=72, Water.Temp=20, Acid.Conc.=85)
#We now apply the predict function and set the predictor variable in the newdata argument. We also set the interval
#type as "predict", and use the default 0.95 confidence level.
predict(stackloss.lm, newdata, interval="predict")
detach(stackloss)    # clean up
#The 95% confidence interval of the stack loss with the given parameters is between 16.466 and 32.697.
chisq.test(lcg_1)
chisq.test(lcg_1)
chisq.test(lcg_2)
chisq.test(mcg_1)
chisq.test(mcg_2)
chisq.test(randu_1)
chisq.test(randu_2)
n0 = rnorm(100,0.1)
n1 = rnorm(500,0.1)
n2 = rnorm(1000,0.1)
n3 = rnorm(5000,0.1)
n4 = rnorm(10000,0.1)
n5 = rnorm(100000,0.1)
n6 = rnorm(1000000,0.1)
hist(n0)
hist(n1)
hist(n2)
hist(n3)
hist(n4)
hist(n5)
hist(n6)
hist(dnorm(n2,0,1))
hist(dnorm(n0+n1+n2+n3+n4+n5+n6,0,1))
pnorm(27.4, mean=50, sd=20)
hist(rnorm(n = 100, mean = 0, sd = 1))
hist(rnorm(n = 500, mean = 0, sd = 1))
hist(rnorm(n = 1000, mean = 0, sd = 1))
hist(rnorm(n = 5000, mean = 0, sd = 1))
hist(rnorm(n = 10000, mean = 0, sd = 1))
hist(rnorm(n = 100000, mean = 0, sd = 1))
hist(rnorm(n = 1000000, mean = 0, sd = 1))
#Folosind functia dnorm(), realizati graficul functiei de densitate de probabilitate a v.a. N(0,1).
curve(dnorm(x),
xlim = c(-4, 4),
ylab = "Density",
main = "Standard Normal Density Function")
#Trasati acest grafic peste histogramele obtinute la pasul anterior.
h <- hist(rnorm(n = 100, mean = 0, sd = 1))
x <- rnorm(n = 100, mean = 0, sd = 1)
xfit<-seq(min(x),max(x),length=4)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
am.glm = glm(formula=am ~ hp + wt, data=mtcars, family=binomial)
am.glm
newdata = data.frame(hp=120, wt=2.8)
newdata
predict(am.glm, newdata, type="response")
am.glm = glm(formula=am ~ hp + wt, data=mtcars,  family=binomial)
am.glm
summary(am.glm)
library(aod)
library(ggplot2)
install.packages(aod)
install.packages("aod")
install.packages("ggplot2")
library(aod)
library(ggplot2)
mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
mydata
head(mydata)
summary(mydata)
sapply(mydata)
sapply(mydata, sd)
xtabs(~admit + rank, data = mydata)
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
confint(mylogit)
# CIs using standard errors
confint.default(mylogit)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6))
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
l <- cbind(0, 0, 0, 1, -1, 0)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = l)
L <- cbind(0, 0, 0, 1, -1, 0)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = L)
# odds ratios only
exp(coef(mylogit))
# odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
newdata1 <- with(mydata, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
## view data frame
newdata1
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
newdata2 <- with(mydata, data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100),
4), gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))
newdata2
newdata3 <- cbind(newdata2, predict(mylogit, newdata = newdata2, type = "link",
se = TRUE))
newdata3 <- within(newdata3, {
PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
## view first few rows of final dataset
head(newdata3)
ggplot(newdata3, aes(x = gre, y = PredictedProb)) + geom_ribbon(aes(ymin = LL,
ymax = UL, fill = rank), alpha = 0.2) + geom_line(aes(colour = rank),
size = 1)
ggplot(newdata3, aes(x = gre, y = PredictedProb)) + geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = 0.2) + geom_line(aes(colour = rank), size = 1)
with(mylogit, null.deviance - deviance)
with(mylogit, df.null - df.residual)
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
logLik(mylogit)
logLik(mylogit)
library(MASS)
head(Pima)
heads(Pima)
train <- data.frame(Pima.tr, Pima.tr2)
Pima.tr
Pima.tr2
train <- data.frame(Pima.tr, Pima.tr2)
train <- data.frame(Pima.tr, Pima.tr2 = unlist(Pima.tr, Pima.tr2))
train <- data.frame(cbind(Pima.tr, Pima.tr2))
train <- as.matrix(Pima.tr, Pima.tr2)
Pima
library(MASS)
Pima.te
Pima.tr
Pima.tr2
train <- data.frame(Pima.tr, Pima.tr2)
train <- data.frame(Pima.tr1, Pima.tr2)
train <- data.frame(Pima.tr, Pima.tr2)
train <- merge(Pima.tr, Pima.tr2)
train
train <- rbind(Pima.tr, Pima.tr2)
train
train <- sub("Yes", "1", type)
train <- sub("Yes", "1", train)
train
train
train <- rbind(Pima.tr, Pima.tr2)
train
train <- sub("Yes", "1", train$type)
train
train <- rbind(Pima.tr, Pima.tr2)
library(plyr)
train <- revalue(train$type, c("Yes"=1))
train <- revalue(train$type, c("No"=0))
head(train)
train <- rbind(Pima.tr, Pima.tr2)
train <- revalue(train$type, c("Yes"=1))
library(plyr)
install.packages("plyr")
library(plyr)
train <- revalue(train$type, c("Yes"=1))
train <- revalue(train$type, c("No"=0))
head(train)
train <- rbind(Pima.tr, Pima.tr2)
library(plyr)
train <- revalue(train$type, c("Yes"=1))
train <- revalue(train$type, c("No"=0))
head(train)
train <- rbind(Pima.tr, Pima.tr2)
library(MASS)
as.numeric(as.factor(train$type))
train
as.numeric(as.factor(train$type))
train <- rbind(Pima.tr, Pima.tr2)
train$type<-ifelse(train$type=='Yes', 1, 0)
heads(train)
train <- rbind(Pima.tr, Pima.tr2)
train$type<-ifelse(train$type=='Yes', 1, 0)
head(train)
train <- rbind(Pima.tr, Pima.tr2)
head(train)
train <- rbind(Pima.tr, Pima.tr2)
train$type<-ifelse(train$type=='Yes', 1, 0)
head(train)
train
sum(is.na(train$type))
pairs(subset(train, select = -c(type)), col = as.factor(train$type))
pairs(subset(train, select = -c(type)), col = as.factor(train$type))
ggplot(type, type), data = train) + geom_line()
ggplot(type, age), data = train) + geom_line()
ggplot(type,age),data = train) + geom_line()
ggplot(aes(x=age,y=type),data=train)+
geom_point(alpha=1/20)+
xlim(13,90)
ggplot(aes(x=type,y=age),data=train)+
geom_point(alpha=1/20)+
xlim(13,90)
ggplot(aes(x=type,y=age),data=train)+
geom_point(alpha=1/20)+
xlim(13,90)
plot(train$age,train$type)
install.packages("e1071")
plot(faithful$waiting, eruption.res, ylab="Residuals", xlab="Waiting Time", main="Old Faithful Eruptions")
plot(train$type, train$age, ylab="Age", xlab="Type", main="Type in functie de age")
plot(train$age, train$type, ylab="Age", xlab="Type", main="Type in functie de age")
plot(train$type, train$age)
plot(train$type, train$age, type="n")
plot(train$type, train$age, type="l")
plot(train$type, train$age, type="h")
plot(train$type, train$age, type="o")
train$age_cat <- ifelse(train$age < 21, "<21",
ifelse((train$age>=21) & (train$age<=25), "21-25",
ifelse((train$age>25) & (dtrain$age<=30), "25-30",
ifelse((train$age>30) & (train$age<=35), "30-35",
ifelse((train$age>35) & (train$age<=40), "35-40",
ifelse((train$age>40) & (train$age<=50), "40-50",
ifelse((train$age>50) & (train$age<=60), "50-60",">60")))))))
train$age_cat <- factor(train$age_cat, levels = c('<21','21-25','25-30','30-35','35-40','40-50','50-60','>60'))
table(train$age_cat)
train$age_cat <- ifelse(train$age < 21, "<21",
ifelse((train$age>=21) & (train$age<=25), "21-25",
ifelse((train$age>25) & (dtrain$age<=30), "25-30",
ifelse((train$age>30) & (train$age<=35), "30-35",
ifelse((train$age>35) & (train$age<=40), "35-40",
ifelse((train$age>40) & (train$age<=50), "40-50",
ifelse((train$age>50) & (train$age<=60), "50-60",">60")))))))
train$age_cat <- ifelse(train$age < 21, "<21", ifelse((train$age>=21) & (train$age<=25), "21-25", ifelse((train$age>25) & (dtrain$age<=30), "25-30", ifelse((train$age>30) & (train$age<=35), "30-35", ifelse((train$age>35) & (train$age<=40), "35-40", ifelse((train$age>40) & (train$age<=50), "40-50", ifelse((train$age>50) & (train$age<=60), "50-60",">60")))))))
train$age_cat <- ifelse(train$age < 21, "<21", ifelse((train$age>=21) & (train$age<=25), "21-25", ifelse((train$age>25) & (train$age<=30), "25-30", ifelse((train$age>30) & (train$age<=35), "30-35", ifelse((train$age>35) & (train$age<=40), "35-40", ifelse((train$age>40) & (train$age<=50), "40-50", ifelse((train$age>50) & (train$age<=60), "50-60",">60")))))))
train$age_cat <- factor(train$age_cat, levels = c('<21','21-25','25-30','30-35','35-40','40-50','50-60','>60'))
table(train$age_cat)
train$age_cat <- ifelse(train$age < 21, "<21",
ifelse((train$age>=21) & (train$age<=25), "21-25",
ifelse((train$age>25) & (train$age<=30), "25-30",
ifelse((train$age>30) & (train$age<=35), "30-35",
ifelse((train$age>35) & (train$age<=40), "35-40",
ifelse((train$age>40) & (train$age<=50), "40-50",
ifelse((train$age>50) & (train$age<=60), "50-60",">60")))))))
train$age_cat <- factor(train$age_cat, levels = c('<21','21-25','25-30','30-35','35-40','40-50','50-60','>60'))
table(train$age)
table(train$age)
train$age_cat <- ifelse(train$age < 21, "<21", ifelse((train$age>=21) & (train$age<=25), "21-25",  ifelse((train$age>25) & (train$age<=30), "25-30",ifelse((train$age>30) & (train$age<=35), "30-35",ifelse((train$age>35) & (train$age<=40), "35-40",ifelse((train$age>40) & (train$age<=50), "40-50",ifelse((train$age>50) & (train$age<=60), "50-60",">60")))))))
train$age_cat <- factor(train$age_cat, levels = c('<21','21-25','25-30','30-35','35-40','40-50','50-60','>60'))
table(train$age)
table(train$age)
library(ggplot2)
ggplot(aes(x = age), data=train) +
geom_histogram(binwidth=1, color='black', fill = "#F79420") +
scale_x_continuous(limits=c(20,90), breaks=seq(20,90,5)) +
xlab("Age") +
ylab("No of people by age")
train
library(ggplot2)
ggplot(aes(x=age_cat, y = bmi), data = train) +
geom_boxplot() +
coord_cartesian(ylim = c(0,70))
library(ggplot2)
ggplot(aes(x=age_cat, y = bmi, z = type), data = train) +
geom_boxplot() +
coord_cartesian(ylim = c(0,70))
library(ggplot2)
ggplot(aes(x=age_cat, y = bmi), data = train) +
geom_boxplot() +
coord_cartesian(ylim = c(0,70))
